Things to work on in future:

##token count feature:
Add cumulative token (input/output/total) tracking over the session.
We already show token use per api call but could be good to also track over the session too.

## session logging feature:
Log session (context, tokens, etc) to a file. Perhaps as a flag on the command
(or default behavior that can be turned off).

## retry request on request error:

We sometimes see roboc exit when a request fails, like this:

Great! Let me re-read and verify Step 2 is complete:
calling tool: grep_files with args: {"pattern": "fn grep_files_decoder", "context": 12}
Request error: error making request

We should both try to log more diagnostics for when this happens and add a retry or two before giving up.

## need more credits on provider
If running low on credits, we will get a message like the below. We should handle it and display an appropriate error. 
{"error":{"message":"This request requires more credits, or fewer max_tokens. You requested up to 64000 tokens, but can only afford 27226. To increase, visit https://openrouter.ai/settings/keys and create a key with a higher total limit","code":402,"metadata":{"provider_name":null}},"user_id":"user_37XVcKibziPjfjRPbI10JUKUlLR"}

Right now we get a decoding error as we don't properly handle the http status code and go right to decoding the body.

json response decode error: DecodeError(expected: Field, found: Nothing path: id)
DecodeError(expected: Field, found: Nothing path: provider)
DecodeError(expected: Field, found: Nothing path: model)
DecodeError(expected: Field, found: Nothing path: object)
DecodeError(expected: Field, found: Nothing path: created)
DecodeError(expected: Field, found: Nothing path: choices)
DecodeError(expected: Field, found: Nothing path: usage)

## write_file tool improvements
When overwriting an existing file, show a diff of old vs new content instead of
just showing the new content. This would make review easier for the user.

## customizable system prompt
Allow users to customize the system prompt sent to the LLM.
